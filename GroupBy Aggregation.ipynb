{"cells":[{"cell_type":"code","source":["Sample_Data=[(1,'Sonu','DA',450000,),\\\n      (2,'Raja','BA',350000,),\\\n      (3,'Monu','QA',550000,),\\\n      (4,'Jit','QA',670000,),\\\n      (5,'Sagar','PMO',950000,),\n      ]\ncolumns=[\"Emp_id\",\"Emp_name\",\"Dept\",\"Salary\"]\n\ndf=spark.createDataFrame(data=Sample_Data,schema=columns)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6308adf9-9059-4136-b6ba-b44314f6ad1f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+--------+----+------+\n|Emp_id|Emp_name|Dept|Salary|\n+------+--------+----+------+\n|     1|    Sonu|  DA|450000|\n|     2|    Raja|  BA|350000|\n|     3|    Monu|  QA|550000|\n|     4|     Jit|  QA|670000|\n|     5|   Sagar| PMO|950000|\n+------+--------+----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["## Aggregate at one column\nfrom pyspark.sql.functions import sum,avg,count,max\ndf.groupBy(\"Dept\")\\\n    .agg(max(\"Salary\").alias(\"Sal\")) \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80614417-517b-4796-8375-4a90bc6a9ead","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+------+\n|Dept|Sal   |\n+----+------+\n|DA  |450000|\n|BA  |350000|\n|QA  |670000|\n|PMO |950000|\n+----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"915b0043-72ff-456d-8ded-78c567bd73a4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"GroupBy Aggregation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3903799444550653}},"nbformat":4,"nbformat_minor":0}
