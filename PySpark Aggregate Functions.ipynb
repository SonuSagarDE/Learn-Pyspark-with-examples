{"cells":[{"cell_type":"code","source":["data=[(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n]\n\ncolumns=(\"emp_name\",\"dept\",\"salary\")\ndf=spark.createDataFrame(data=data,schema=columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cb6a00d9-d96e-4fb3-b191-0863f55bfb04","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a6316f6a-a0b0-41fa-a341-134b1bc03d28","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_name: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+--------+---------+------+\n|emp_name|     dept|salary|\n+--------+---------+------+\n|   James|    Sales|  3000|\n| Michael|    Sales|  4600|\n|  Robert|    Sales|  4100|\n|   Maria|  Finance|  3000|\n|   James|    Sales|  3000|\n|   Scott|  Finance|  3300|\n|     Jen|  Finance|  3900|\n|    Jeff|Marketing|  3000|\n|   Kumar|Marketing|  2000|\n|    Saif|    Sales|  4100|\n+--------+---------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["##approx_count_distinct Aggregate Function  returns the count of distinct items in a group.\nfrom pyspark.sql.functions import approx_count_distinct\nprint(\"approx_count_distinct :\"+ \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8fd7b263-52a9-47e0-b0de-e7c8ebf47157","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["approx_count_distinct :6\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ae06b251-25db-40eb-aa5d-2f0f0fca9126","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Aggregate Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
