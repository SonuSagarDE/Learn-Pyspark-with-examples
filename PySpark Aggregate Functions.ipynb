{"cells":[{"cell_type":"code","source":["data=[(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n]\n\ncolumns=(\"emp_name\",\"dept\",\"salary\")\ndf=spark.createDataFrame(data=data,schema=columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cb6a00d9-d96e-4fb3-b191-0863f55bfb04","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a6316f6a-a0b0-41fa-a341-134b1bc03d28","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_name: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+--------+---------+------+\n|emp_name|     dept|salary|\n+--------+---------+------+\n|   James|    Sales|  3000|\n| Michael|    Sales|  4600|\n|  Robert|    Sales|  4100|\n|   Maria|  Finance|  3000|\n|   James|    Sales|  3000|\n|   Scott|  Finance|  3300|\n|     Jen|  Finance|  3900|\n|    Jeff|Marketing|  3000|\n|   Kumar|Marketing|  2000|\n|    Saif|    Sales|  4100|\n+--------+---------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["##approx_count_distinct Aggregate Function  returns the count of distinct items in a group.\nfrom pyspark.sql.functions import approx_count_distinct\nprint(\"approx_count_distinct :\"+ \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8fd7b263-52a9-47e0-b0de-e7c8ebf47157","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["approx_count_distinct :6\n"]}],"execution_count":0},{"cell_type":"code","source":["## avg functions\n\nfrom pyspark.sql.functions import avg\nprint(\"avg_salary:\"+ \\\n     str(df.select(avg(\"salary\")).collect()[0][0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"27a85f11-376b-4ccc-9be3-3aa24e2ad671","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["avg_salary:3400.0\n"]}],"execution_count":0},{"cell_type":"code","source":["## collect_list function returns all values from an input columns with duplicates\nfrom pyspark.sql.functions import collect_list\ndf.select(collect_list(\"salary\")).show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d900db76-6bf9-42cf-8618-b43744c287c6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------------------------------------------------+\n|collect_list(salary)                                        |\n+------------------------------------------------------------+\n|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n+------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["## collect_set function return all values from an input columns without duplicates\n\nfrom pyspark.sql.functions import collect_set\ndf.select(collect_set(\"salary\")).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"794732a7-fc1e-45cd-83c1-b293a937ff79","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------------------------+\n|collect_set(salary)                 |\n+------------------------------------+\n|[4600, 3000, 3900, 4100, 3300, 2000]|\n+------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f9bb9ca-22cc-4431-b5fd-34e58713154a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Aggregate Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
